{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "790eee2b-19c9-46f4-9bfc-4555993078ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow: 2.20.0\n",
      "GPU available: No (CPU only)\n",
      "Data file: C:\\Users\\TIK03\\Documents\\GitHub\\DIT5411-ProjectAssignmen\\data\\daily_HKO_GMT_ALL.csv\n",
      "Output root: C:\\Users\\TIK03\\Documents\\GitHub\\DIT5411-ProjectAssignmen\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\TIK03\\\\Documents\\\\GitHub\\\\DIT5411-ProjectAssignmen\\\\data\\\\daily_HKO_GMT_ALL.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 84\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTemperature range (train): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemp\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmin()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m°C to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemp\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m°C\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m train_df, test_df\n\u001b[1;32m---> 84\u001b[0m train_df, test_df \u001b[38;5;241m=\u001b[39m load_and_preprocess(DATA_FILE)\n\u001b[0;32m     86\u001b[0m \u001b[38;5;66;03m# quick plot of raw data\u001b[39;00m\n\u001b[0;32m     87\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m14\u001b[39m,\u001b[38;5;241m5\u001b[39m))\n",
      "Cell \u001b[1;32mIn[9], line 52\u001b[0m, in \u001b[0;36mload_and_preprocess\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_and_preprocess\u001b[39m(file_path):\n\u001b[1;32m---> 52\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\n\u001b[0;32m     53\u001b[0m         file_path,\n\u001b[0;32m     54\u001b[0m         header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     55\u001b[0m         names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYear\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMonth\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDay\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemp\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFlag\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     56\u001b[0m         na_values\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m---\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     57\u001b[0m         skipinitialspace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     58\u001b[0m         engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     59\u001b[0m     )\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;66;03m# Create Date\u001b[39;00m\n\u001b[0;32m     61\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYear\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMonth\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDay\u001b[39m\u001b[38;5;124m'\u001b[39m]], errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoerce\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\TIK03\\\\Documents\\\\GitHub\\\\DIT5411-ProjectAssignmen\\\\data\\\\daily_HKO_GMT_ALL.csv'"
     ]
    }
   ],
   "source": [
    "import os, warnings, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "tf.random.set_seed(42); np.random.seed(42)\n",
    "print(\"TF:\", tf.__version__)\n",
    "print(\"GPU:\", \"Available\" if tf.config.list_physical_devices('GPU') else \"CPU only\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# CELL 1: CONFIG + Paths\n",
    "# -------------------------------------------------\n",
    "DATA_FILE = Path(r\"C:\\Users\\TIK03\\Documents\\GitHub\\DIT5411-ProjectAssignment\\data\\daily_HKO_GMT_ALL.csv\")\n",
    "NOTEBOOK_ROOT = Path(r\"C:\\Users\\TIK03\\Documents\\GitHub\\DIT5411-ProjectAssignment\")\n",
    "SEQ_LEN = 30  # Days to look back\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 32\n",
    "print(f\"Data: {DATA_FILE}\")\n",
    "DATA_FILE.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# CELL 2: Load & Preprocess Data\n",
    "# -------------------------------------------------\n",
    "def load_and_preprocess(file_path):\n",
    "    df = pd.read_csv(\n",
    "        file_path,\n",
    "        header=None,\n",
    "        names=['Year', 'Month', 'Day', 'temp', 'Flag'], \n",
    "        na_values=[\"---\"], \n",
    "        skipinitialspace=True,\n",
    "        engine='python'\n",
    "    )\n",
    "    \n",
    "    print(\"First few raw rows after loading:\")\n",
    "    print(df.head(5))\n",
    "    \n",
    "    # Create Date column\n",
    "    df['Date'] = pd.to_datetime(df[['Year', 'Month', 'Day']], errors='coerce')\n",
    "    \n",
    "    # Drop rows with invalid dates (should be none)\n",
    "    df = df.dropna(subset=['Date'])\n",
    "    \n",
    "    # Keep only Date and temperature\n",
    "    df = df[['Date', 'temp']].set_index('Date').sort_index()\n",
    "    \n",
    "    # Convert temp to numeric (in case any weird values)\n",
    "    df['temp'] = pd.to_numeric(df['temp'], errors='coerce')\n",
    "    \n",
    "    # Remove any remaining NaN temperatures\n",
    "    df = df.dropna(subset=['temp'])\n",
    "    \n",
    "    # Remove duplicate dates (keep first)\n",
    "    df = df[~df.index.duplicated(keep='first')]\n",
    "    \n",
    "    # Subset the required periods\n",
    "    train_df = df['1980-01-01':'2024-12-31']\n",
    "    test_df  = df['2025-01-01':'2025-10-30']\n",
    "    \n",
    "    # Fill any tiny gaps (should be almost none)\n",
    "    train_df = train_df.interpolate(method='linear').ffill().bfill()\n",
    "    test_df  = test_df.interpolate(method='linear').ffill().bfill()\n",
    "    \n",
    "    print(f\"\\nSuccessfully loaded!\")\n",
    "    print(f\"Full data range: {df.index.min().date()} to {df.index.max().date()}\")\n",
    "    print(f\"Train (1980–2024): {len(train_df)} days\")\n",
    "    print(f\"Test (2025 Jan–Oct): {len(test_df)} days\")\n",
    "    print(f\"Temperature range: {train_df['temp'].min():.1f}°C to {train_df['temp'].max():.1f}°C\")\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "train_df, test_df = load_and_preprocess(DATA_FILE)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "train_df['temp'].plot(ax=ax, label='Train (1980–2024)', color='blue', alpha=0.8)\n",
    "test_df['temp'].plot(ax=ax, label='Test (2025)', color='red', linewidth=2)\n",
    "ax.set_ylabel('Grass Minimum Temperature (°C)')\n",
    "ax.set_title('Hong Kong Daily Grass Minimum Temperature (HKO)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(NOTEBOOK_ROOT / 'raw_data.png', dpi=200)\n",
    "plt.show()\n",
    "# -------------------------------------------------\n",
    "# CELL 3: Create Sequences\n",
    "# -------------------------------------------------\n",
    "def create_sequences(data, seq_len):\n",
    "    \"\"\"\n",
    "    data: numpy array of shape (n_samples, 1)  ← scaled temperature\n",
    "    returns X: (samples, seq_len, 1), y: (samples, 1)\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(seq_len, len(data)):\n",
    "        X.append(data[i-seq_len:i])\n",
    "        y.append(data[i])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "train_scaled = scaler.fit_transform(train_df)\n",
    "test_scaled  = scaler.transform(test_df)\n",
    "\n",
    "X_train, y_train = create_sequences(train_scaled, SEQ_LEN)\n",
    "X_test,  y_test  = create_sequences(test_scaled,  SEQ_LEN)\n",
    "\n",
    "print(f\"Sequences created!\")\n",
    "print(f\"X_train: {X_train.shape} → (samples, {SEQ_LEN}, 1)\")\n",
    "print(f\"X_test : {X_test.shape}\")\n",
    "print(f\"y_train/y_test dtype: {y_train.dtype} (should be float32/64)\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# CELL 4: Build Models\n",
    "# -------------------------------------------------\n",
    "def build_rnn(seq_len, n_features=1):\n",
    "    model = models.Sequential([\n",
    "        layers.SimpleRNN(50, activation='relu', input_shape=(seq_len, n_features), return_sequences=True),\n",
    "        layers.SimpleRNN(50, activation='relu'),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "def build_lstm(seq_len, n_features=1, bidirectional=False):\n",
    "    model = models.Sequential()\n",
    "    if bidirectional:\n",
    "        model.add(layers.Bidirectional(layers.LSTM(50, return_sequences=True), input_shape=(seq_len, n_features)))\n",
    "        model.add(layers.Bidirectional(layers.LSTM(50)))\n",
    "    else:\n",
    "        model.add(layers.LSTM(50, return_sequences=True, input_shape=(seq_len, n_features)))\n",
    "        model.add(layers.LSTM(50))\n",
    "    model.add(layers.Dropout(0.2))  # Regularization\n",
    "    model.add(layers.Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "# -------------------------------------------------\n",
    "# CELL 5: Train Models\n",
    "# -------------------------------------------------\n",
    "results = {}\n",
    "models_dict = {}\n",
    "\n",
    "for name, build_fn in [('RNN', build_rnn), ('LSTM', build_lstm)]:\n",
    "    print(f\"\\n=== TRAINING {name} ===\")\n",
    "    model = build_fn(SEQ_LEN)\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        validation_split=0.1,\n",
    "        verbose=1\n",
    "    )\n",
    "    models_dict[name] = model\n",
    "    model.save(NOTEBOOK_ROOT / f'{name.lower()}_model.keras')\n",
    "    \n",
    "    best_epoch = np.argmin(history.history['val_loss'])\n",
    "    results[name] = {'best_epoch': best_epoch + 1, 'min_val_loss': min(history.history['val_loss'])}\n",
    "    print(f\"{name} → Best Epoch: {best_epoch+1}, Val Loss: {results[name]['min_val_loss']:.4f}\")\n",
    "\n",
    "print(\"\\n=== TRAINING BIDIR LSTM (Extension) ===\")\n",
    "bidir_model = build_lstm(SEQ_LEN, bidirectional=True)\n",
    "bidir_history = bidir_model.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_split=0.1, verbose=1)\n",
    "models_dict['BiLSTM'] = bidir_model\n",
    "bidir_model.save(NOTEBOOK_ROOT / 'bilstm_model.keras')\n",
    "results['BiLSTM'] = {'best_epoch': np.argmin(bidir_history.history['val_loss']) + 1,\n",
    "                     'min_val_loss': min(bidir_history.history['val_loss'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350e4d72-59de-4646-be80-07014db2cc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------\n",
    "# CELL 6: Evaluate & Visualize (FIXED FOR 3 MODELS)\n",
    "# -------------------------------------------------\n",
    "def evaluate_model(model, X_test, y_test, scaler):\n",
    "    y_pred_scaled = model.predict(X_test, verbose=0)\n",
    "    y_pred = scaler.inverse_transform(y_pred_scaled)\n",
    "    y_true = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    return y_true, y_pred, mae, rmse\n",
    "\n",
    "# Create 3 subplots in a row (cleaner than 2x2)\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.flatten()  # Make it easy to index\n",
    "\n",
    "for i, name in enumerate(results):  # RNN, LSTM, BiLSTM → i = 0,1,2\n",
    "    model = models_dict[name]\n",
    "    y_true, y_pred, mae, rmse = evaluate_model(model, X_test, y_test, scaler)\n",
    "    \n",
    "    # Update results\n",
    "    results[name]['MAE'] = mae\n",
    "    results[name]['RMSE'] = rmse\n",
    "    print(f\"{name}: MAE={mae:.3f}°C, RMSE={rmse:.3f}°C\")\n",
    "    \n",
    "    # Plot actual vs predicted (first 200 days of 2025)\n",
    "    ax1 = axes[i]\n",
    "    ax1.plot(y_true[:200], label='Actual', color='tab:blue', alpha=0.8)\n",
    "    ax1.plot(y_pred[:200], label='Predicted', color='tab:orange', alpha=0.8)\n",
    "    ax1.set_title(f'{name} – Actual vs Predicted (2025)', fontsize=14)\n",
    "    ax1.set_ylabel('Temperature (°C)')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Residuals histogram\n",
    "    ax2 = axes[i + 3]\n",
    "    residuals = y_true[:200] - y_pred[:200]\n",
    "    ax2.hist(residuals, bins=25, color='lightcoral', alpha=0.7, edgecolor='black')\n",
    "    ax2.axvline(0, color='red', linestyle='--')\n",
    "    ax2.set_title(f'{name} – Residuals (MAE = {mae:.3f}°C)')\n",
    "    ax2.set_xlabel('Prediction Error (°C)')\n",
    "\n",
    "# Hide empty 6th subplot (we only use 6)\n",
    "for j in range(6, 6):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(NOTEBOOK_ROOT / 'predictions_comparison.png', dpi=200, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5e3d82-e28a-4037-b6e1-fddac13d7c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------\n",
    "# CELL 7: Final Analysis & Table\n",
    "# -------------------------------------------------\n",
    "# Fix winter analysis (y_true/y_pred are now inside the loop → recalculate for best model)\n",
    "best_model_name = min(results, key=lambda k: results[k]['MAE'])\n",
    "print(f\"\\nBest model by MAE: {best_model_name}\")\n",
    "\n",
    "# Re-predict with best model for winter analysis\n",
    "best_model = models_dict[best_model_name]\n",
    "y_true_full, y_pred_full, _, _ = evaluate_model(best_model, X_test, y_test, scaler)\n",
    "\n",
    "# Winter months in test set (Jan–Feb 2025 + Dec if any)\n",
    "winter_mask = test_df.index.month.isin([12, 1, 2])\n",
    "valid_winter = winter_mask[SEQ_LEN:]  # align with predictions\n",
    "winter_true = y_true_full[valid_winter]\n",
    "winter_pred = y_pred_full[valid_winter]\n",
    "\n",
    "if len(winter_true) > 0:\n",
    "    winter_mae = mean_absolute_error(winter_true, winter_pred)\n",
    "    print(f\"Winter (Dec–Feb 2025 MAE: {winter_mae:.3f}°C ← higher error during cold snaps\")\n",
    "\n",
    "# Final comparison table\n",
    "comparison_df = pd.DataFrame(results).T[['MAE', 'RMSE']].round(3)\n",
    "comparison_df = comparison_df.sort_values('MAE')\n",
    "print(\"\\nFinal Model Comparison:\")\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d74f2f8-14cd-4818-90e7-dcbd205edc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------\n",
    "# CELL 8: INTERACTIVE FORECASTING EXPERIMENTER\n",
    "# -------------------------------------------------\n",
    "def run_forecast_experiment(sequence_length=30):\n",
    "    \"\"\"\n",
    "    Type any number (e.g. 30, 60, 90) and this function will:\n",
    "    - Rebuild sequences with that lookback\n",
    "    - Retrain RNN, LSTM, BiLSTM\n",
    "    - Show results + plots\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"RUNNING EXPERIMENT WITH SEQUENCE LENGTH = {sequence_length} DAYS\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    global X_train, y_train, X_test, y_test, results, models_dict, comparison_df\n",
    "    \n",
    "    # 1. Create new sequences\n",
    "    X_train, y_train = create_sequences(train_scaled, sequence_length)\n",
    "    X_test,  y_test  = create_sequences(test_scaled,  sequence_length)\n",
    "    print(f\"New sequences → Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "    \n",
    "    # 2. Rebuild & retrain models\n",
    "    results = {}\n",
    "    models_dict = {}\n",
    "    \n",
    "    for name, build_fn in [('RNN', build_rnn), ('LSTM', build_lstm)]:\n",
    "        print(f\"\\nTraining {name}...\")\n",
    "        model = build_fn(sequence_length)  # pass new seq_len\n",
    "        history = model.fit(X_train, y_train,\n",
    "                            epochs=EPOCHS,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            validation_split=0.1,\n",
    "                            verbose=0)  # quiet mode\n",
    "        models_dict[name] = model\n",
    "        val_loss = min(history.history['val_loss'])\n",
    "        results[name] = {'min_val_loss': val_loss}\n",
    "        print(f\"   {name} → Val Loss: {val_loss:.5f}\")\n",
    "    \n",
    "    # BiLSTM\n",
    "    print(f\"\\nTraining BiLSTM...\")\n",
    "    model = build_lstm(sequence_length, bidirectional=True)\n",
    "    history = model.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_split=0.1, verbose=0)\n",
    "    models_dict['BiLSTM'] = model\n",
    "    val_loss = min(history.history['val_loss'])\n",
    "    results['BiLSTM'] = {'min_val_loss': val_loss}\n",
    "    print(f\"   BiLSTM → Val Loss: {val_loss:.5f}\")\n",
    "    \n",
    "    # 3. Evaluate all\n",
    "    print(f\"\\nEvaluating on 2025 test set...\")\n",
    "    for name in models_dict:\n",
    "        y_true, y_pred, mae, rmse = evaluate_model(models_dict[name], X_test, y_test, scaler)\n",
    "        results[name]['MAE'] = mae\n",
    "        results[name]['RMSE'] = rmse\n",
    "    \n",
    "    # 4. Show results table\n",
    "    comparison_df = pd.DataFrame(results).T[['MAE', 'RMSE']].round(3)\n",
    "    comparison_df = comparison_df.sort_values('MAE')\n",
    "    print(f\"\\nRESULTS (Sequence Length = {sequence_length} days)\")\n",
    "    print(\"-\" * 40)\n",
    "    print(comparison_df)\n",
    "    print(f\"\\nBest Model: {comparison_df.index[0]} (MAE = {comparison_df.iloc[0,0]:.3f}°C)\")\n",
    "    \n",
    "    # 5. Quick plot of best model\n",
    "    best_name = comparison_df.index[0]\n",
    "    y_true, y_pred, _, _ = evaluate_model(models_dict[best_name], X_test, y_test, scaler)\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.plot(y_true[:200], label='Actual', color='tab:blue', linewidth=2)\n",
    "    plt.plot(y_pred[:200], label=f'Predicted ({best_name})', color='tab:orange', linewidth=2)\n",
    "    plt.title(f'Best Model: {best_name} | Seq Len = {sequence_length} days | MAE = {comparison_df.iloc[0,0]:.3f}°C')\n",
    "    plt.ylabel('Grass Min Temp (°C)')\n",
    "    plt.xlabel('Days into 2025')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
